# LLM-FineTune-Chatbot

---

This project aims to fine-tune the `Mistral-7B-Instruct-v0.2` model using methods such as LoRA (Low-Rank Adaptation) and 4-bit quantization with Hugging Face Transformers and related libraries. The primary goal is to gain experience in the field of LLM and transformers and to learn the optimization processes of these models. The project was developed in the Google Colab environment.

---

## Contents

- [Methods Used](#methods-used)
- [Installation and Requirements](#installation-and-requirements)
- [Model Training](#model-training)
- [Usage of the Model](#usage-of-the-model)
- [Performance Examples](#performance-examples)
- [Code Development Process](#code-development-process)
- [Performance Observations](#performance-observations)
- [ðŸ“œ License Information](#license-information)

---

## Methods Used

- **LoRA (Low-Rank Adaptation):**  
  Allows updating large weight matrices with low-rank decompositions, significantly reducing the number of parameters during fine-tuning.

- **4-bit Quantization (NF4):**  
  Applied using the BitsAndBytes library, this method reduces the model's memory footprint and enables fast computations.

- **Gradient Checkpointing:**  
  Saves memory by recomputing intermediate values during training, enabling the training of very large models.

- **Paged AdamW Optimizer (32-bit precision):**  
  Executes model updates with 32-bit precision, improving memory management during the training process.

- **Causal Language Modeling:**  
  Performs autoregressive language modeling by predicting future tokens based on the given input.

- **Supervised Fine-Tuning (SFT):**  
  Supervised training on the Wikitext-103 dataset enhances the model's natural language generation quality.

- **Tokenizer and Data Preprocessing:**  
  Processes text data using a tokenizer compatible with the DistilBERT format to make it suitable for the model.

- **Memory-Efficient Training:**  
  The `prepare_model_for_kbit_training` function optimizes the model for k-bit training.

---

## Installation and Requirements

To run the project, the following libraries need to be installed. If you are using Google Colab, you can install the required packages via the Colab cells:

```bash
!pip install -q -U transformers bitsandbytes peft datasets trl
```

---

## Model Training

The model training process was carried out using the `build_model.ipynb` notebook. The main steps of the training process are summarized below:

1. **Loading the Model and Tokenizer:**
   - The `mistralai/Mistral-7B-Instruct-v0.2` model was loaded from Hugging Face, and an appropriate `tokenizer` was configured.
   - The EOS token was set as the padding token.

2. **Quantization and Memory Optimization:**
   - 4-bit quantization (NF4) was applied using the BitsAndBytes library.
   - Gradient checkpointing was enabled to optimize memory usage.
   - The model was prepared for low-bit training using the `prepare_model_for_kbit_training` function.

3. **Dataset and Preprocessing:**
   - A specific portion of the Wikitext-103 dataset was loaded for training and evaluation.
   - A simple `generate_prompt` function was used to format the data for the model.

4. **Fine-Tuning with LoRA:**
   - LoRA configuration was applied to enhance parameter efficiency in specific layers of the model.
   - Training was conducted using the Paged AdamW optimizer with 32-bit precision.

5. **Training Process:**
   - The model was trained and evaluated using the `SFTTrainer`.
   - Once training was completed, the model was uploaded to the Hugging Face Hub.

---

## Usage of the Model

The `sample_speech.ipynb` notebook demonstrates how to use the fine-tuned model. In this notebook:

- **Model Loading:**  
  The fine-tuned model is loaded by merging it with the base model using PEFT.

- **Interactive Chatbot:**  
  An interactive chatbot function (`chatbotStart`) is provided for engaging in dialogues with the user.

- **Text Generation of Varying Lengths:**  
  The user is offered options for generating short, medium, long, and very long texts, allowing observation of the modelâ€™s performance across various scenarios.

- **Custom Stopping Criteria:**  
  A custom `StoppingCriteria` class has been implemented to halt generation when specific punctuation marks are reached in the generated text.

The user can run the `sample_speech.ipynb` file to engage in interactive dialogues with the model and test its performance across different text lengths.

---

## Performance Examples

In the `sample_speech.ipynb` notebook, text examples generated by the model were tested with different length options:

- **Short Text (max_new_tokens = 100)**
- **Medium Text (max_new_tokens = 250)**
- **Long Text (max_new_tokens = 500)**
- **Very Long Text (max_new_tokens = 20000)**

In these examples, the model produced coherent, consistent, and fluent outputs that matched the input. In particular, the stopping criteria after punctuation marks helped maintain the natural flow of language.

---

## Code Development Process

The code structure of this project was developed with an emphasis on modularity and readability. Key points during the development process include:

- **Modular Notebooks:**  
  - `build_model.ipynb`: Describes the model configuration, fine-tuning settings, data preprocessing, and training processes with accompanying comments.  
  - `sample_speech.ipynb`: Provides an interactive chatbot application for using the fine-tuned model and conducting tests with various text lengths.

- **Optimization and Memory Management:**  
  Thanks to the applied 4-bit quantization, gradient checkpointing, and the `prepare_model_for_kbit_training` function, an efficient training process was achieved even on limited hardware resources.

- **Detailed Comments and Experimental Approach:**  
  Each code block includes detailed comments explaining the methods used and the reasons behind their selection, which facilitates future improvements and debugging.

- **Iterative Improvement Process:**  
  Different hyperparameter settings and stopping criteria were experimented with during the training process, continuously optimizing the model's output quality and efficiency.

---

## Performance Observations

Based on the tests conducted in the `sample_speech.ipynb` notebook:

- **Short and Medium Text Generation:**  
  The model is capable of producing coherent, fluent, and contextually appropriate outputs for short and medium-length texts.

- **Long and Very Long Text Generation:**  
  For long text generation, the model has generally achieved successful results despite certain limitations. The stopping criteria after punctuation marks supported the natural flow of the texts.

- **Impact of Optimization:**  
  Techniques such as 4-bit quantization and gradient checkpointing allowed efficient use of hardware resources during training and inference, leading to fast and effective results.

Overall, the fine-tuned model demonstrates adequate performance for interactive chatbot applications, and further experimental work on LLM and transformer models could lead to further improvements.

---

## ðŸ“œ License Information

The dataset and model used in this project are provided under the following licenses:

- **wikitext-103-raw-v1** â†’ *Creative Commons Attribution-ShareAlike 3.0 (CC BY-SA 3.0)*  
  When using this dataset, appropriate attribution must be given, and derivative works must be shared under the same license.

- **Mistral-7B-Instruct-v0.2** â†’ *Apache License 2.0*  
  This model is provided under the Apache License 2.0. Its usage, modification, and distribution are free; however, the license text must be included in the project and any modifications must be documented.

---
